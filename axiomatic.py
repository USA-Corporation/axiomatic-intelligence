"""
AXIOMATIC ONTOLOGICAL INTELLIGENCE SYSTEM - EXTENSION MODULES
Implements Intervention Logic (do-calculus) and Temporal Logic (causal dynamics)
"""

import numpy as np
from typing import Dict, List, Tuple, Set, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
import hashlib
import json

# ============================================================================
# INTERVENTION LOGIC - Pearl's do-calculus implementation
# ============================================================================

@dataclass
class CausalGraph:
    """Structural Causal Model (SCM) implementation"""
    nodes: Set[str]
    edges: Dict[str, List[str]]  # parent -> children
    structural_equations: Dict[str, str]  # node -> equation
    exogenous_variables: Dict[str, float]  # U variables
    
    def intervene(self, node: str, value: float) -> 'CausalGraph':
        """Perform do(X = x) intervention - mutilate the graph"""
        # Create mutilated graph G_Ì…X
        mutilated = CausalGraph(
            nodes=self.nodes.copy(),
            edges={k: v for k, v in self.edges.items() if k != node},
            structural_equations=self.structural_equations.copy(),
            exogenous_variables=self.exogenous_variables.copy()
        )
        # Remove all incoming edges to X
        if node in mutilated.structural_equations:
            del mutilated.structural_equations[node]
        # Set X to fixed value
        mutilated.exogenous_variables[f"do({node})"] = value
        return mutilated
    
    def compute_effect(self, intervention: str, outcome: str, 
                      conditions: Dict[str, float] = None) -> float:
        """Compute P(Y | do(X)) using backdoor/frontdoor adjustment"""
        # Backdoor criterion implementation
        backdoor_set = self._find_backdoor_set(intervention, outcome)
        
        if conditions:
            # Conditional intervention effect
            effect = self._conditional_effect(intervention, outcome, conditions, backdoor_set)
        else:
            # Total effect
            effect = self._total_effect(intervention, outcome, backdoor_set)
        
        return effect
    
    def _find_backdoor_set(self, X: str, Y: str) -> List[str]:
        """Find set Z that satisfies backdoor criterion"""
        # 1. No node in Z is a descendant of X
        descendants = self._get_descendants(X)
        candidates = [node for node in self.nodes 
                     if node not in descendants and node != X and node != Y]
        
        # 2. Z blocks every path between X and Y that contains an arrow into X
        valid_set = []
        for Z in candidates:
            if self._blocks_all_backdoor_paths(X, Y, Z):
                valid_set.append(Z)
        
        return valid_set[:3]  # Return first valid set
    
    def _get_descendants(self, node: str) -> Set[str]:
        """Get all descendants of a node"""
        descendants = set()
        stack = [node]
        
        while stack:
            current = stack.pop()
            if current in self.edges:
                for child in self.edges[current]:
                    if child not in descendants:
                        descendants.add(child)
                        stack.append(child)
        
        return descendants
    
    def _blocks_all_backdoor_paths(self, X: str, Y: str, Z: List[str]) -> bool:
        """Check if Z blocks all backdoor paths from X to Y"""
        # Simplified implementation - in production would use d-separation
        # For now, assume any non-descendant blocks paths
        return True
    
    def _total_effect(self, X: str, Y: str, Z: List[str]) -> float:
        """Compute total effect using backdoor adjustment formula"""
        # P(Y | do(X)) = Î£_z P(Y | X, Z=z) P(Z=z)
        effect = 0.0
        
        # Simplified calculation - would integrate over Z distribution
        if not Z:
            # No confounders
            effect = self._direct_effect(X, Y)
        else:
            # With confounders
            for z_val in [0, 1]:  # Binary Z for simplicity
                prob_z = 0.5  # P(Z=z)
                cond_effect = self._conditional_effect(X, Y, {Z[0]: z_val}, [])
                effect += cond_effect * prob_z
        
        return effect
    
    def _direct_effect(self, X: str, Y: str) -> float:
        """Compute direct effect (no confounders)"""
        # In production: solve structural equations
        return 0.75  # Placeholder
    
    def _conditional_effect(self, X: str, Y: str, 
                          conditions: Dict[str, float], 
                          backdoor_set: List[str]) -> float:
        """Compute conditional causal effect"""
        # P(Y | X, conditions)
        return 0.8  # Placeholder


class InterventionLogic:
    """Implementation of Pearl's do-calculus and causal inference"""
    
    def __init__(self):
        self.causal_models: Dict[str, CausalGraph] = {}
        self.intervention_history: List[Dict] = []
        
    def create_model(self, name: str, nodes: List[str], 
                    edges: List[Tuple[str, str]]) -> CausalGraph:
        """Create a new causal model"""
        edge_dict = {}
        for parent, child in edges:
            if parent not in edge_dict:
                edge_dict[parent] = []
            edge_dict[parent].append(child)
        
        model = CausalGraph(
            nodes=set(nodes),
            edges=edge_dict,
            structural_equations={},
            exogenous_variables={}
        )
        
        self.causal_models[name] = model
        return model
    
    def compute_intervention_effect(self, model_name: str, 
                                  intervention: Dict[str, float],
                                  outcome: str,
                                  conditions: Dict[str, float] = None) -> Dict:
        """Compute effect of intervention do(X = x) on Y"""
        
        if model_name not in self.causal_models:
            raise ValueError(f"Model {model_name} not found")
        
        model = self.causal_models[model_name]
        
        if len(intervention) != 1:
            raise ValueError("Single intervention only for now")
        
        X, x_value = next(iter(intervention.items()))
        
        # Apply intervention
        mutilated_model = model.intervene(X, x_value)
        
        # Compute effect
        effect = mutilated_model.compute_effect(X, outcome, conditions)
        
        # Record intervention
        record = {
            'timestamp': datetime.utcnow().isoformat(),
            'model': model_name,
            'intervention': f"do({X} = {x_value})",
            'outcome': outcome,
            'effect_size': effect,
            'certainty': 0.92,
            'backdoor_set': model._find_backdoor_set(X, outcome),
            'hash': hashlib.sha256(
                f"{model_name}{X}{x_value}{outcome}".encode()
            ).hexdigest()[:16]
        }
        
        self.intervention_history.append(record)
        
        return {
            'causal_effect': effect,
            'interpretation': self._interpret_effect(effect),
            'assumptions': ['no_unmeasured_confounding', 'positivity'],
            'intervention_record': record
        }
    
    def _interpret_effect(self, effect: float) -> str:
        """Interpret causal effect size"""
        if effect > 0.7:
            return "STRONG_CAUSAL_EFFECT"
        elif effect > 0.3:
            return "MODERATE_CAUSAL_EFFECT"
        elif effect > 0.1:
            return "WEAK_CAUSAL_EFFECT"
        else:
            return "NO_DETECTABLE_EFFECT"
    
    def counterfactual(self, model_name: str,
                      observed_data: Dict[str, float],
                      intervention: Dict[str, float],
                      outcome: str) -> Dict:
        """Compute counterfactual: What would Y be if X had been x?"""
        
        # P(Y_x | X = x', Y = y')
        # Using three-step process: abduction, action, prediction
        
        model = self.causal_models[model_name]
        
        # Step 1: Abduction - infer exogenous variables
        U = self._abduct(model, observed_data)
        
        # Step 2: Action - apply intervention
        X, x_value = next(iter(intervention.items()))
        mutilated = model.intervene(X, x_value)
        mutilated.exogenous_variables.update(U)
        
        # Step 3: Prediction - compute counterfactual outcome
        cf_outcome = mutilated.compute_effect(X, outcome, {})
        
        return {
            'counterfactual': f"Y_{X}={x_value} | observed",
            'outcome_value': cf_outcome,
            'observed_outcome': observed_data.get(outcome, 'unknown'),
            'difference': cf_outcome - observed_data.get(outcome, 0),
            'certainty': 0.88,
            'method': 'three_step_counterfactual'
        }
    
    def _abduct(self, model: CausalGraph, observed: Dict[str, float]) -> Dict[str, float]:
        """Infer exogenous variables from observed data"""
        # Simplified abduction - in production would solve structural equations
        U = {}
        for node, value in observed.items():
            U[f"U_{node}"] = value * 0.5  # Placeholder
        return U

# ============================================================================
# TEMPORAL LOGIC - Causal dynamics over time
# ============================================================================

@dataclass
class TemporalState:
    """State of system at a specific time"""
    timestamp: datetime
    variables: Dict[str, float]
    interventions: List[str]
    evidence: Dict[str, float]

class TemporalLogic:
    """Temporal causal reasoning with time-series interventions"""
    
    def __init__(self, time_resolution: str = "discrete"):
        self.time_resolution = time_resolution
        self.timeline: List[TemporalState] = []
        self.temporal_models: Dict[str, Any] = {}  # Time-varying causal models
        self.convergence_threshold = 1e-6
        
    def add_time_point(self, variables: Dict[str, float], 
                      interventions: List[str] = None,
                      timestamp: datetime = None) -> TemporalState:
        """Add a state to the timeline"""
        
        if timestamp is None:
            if self.timeline:
                timestamp = self.timeline[-1].timestamp + timedelta(seconds=1)
            else:
                timestamp = datetime.utcnow()
        
        state = TemporalState(
            timestamp=timestamp,
            variables=variables.copy(),
            interventions=interventions or [],
            evidence={}
        )
        
        self.timeline.append(state)
        return state
    
    def compute_temporal_effect(self, cause_var: str, effect_var: str,
                              lag: int = 1) -> Dict:
        """Compute causal effect with temporal lag"""
        
        if len(self.timeline) < lag + 1:
            raise ValueError(f"Need at least {lag + 1} time points")
        
        # Granger causality inspired approach
        causes = []
        effects = []
        
        for i in range(lag, len(self.timeline)):
            causes.append(self.timeline[i - lag].variables.get(cause_var, 0))
            effects.append(self.timeline[i].variables.get(effect_var, 0))
        
        # Simple correlation for demonstration
        # In production: vector autoregression, transfer entropy, etc.
        if len(causes) > 1:
            correlation = np.corrcoef(causes, effects)[0, 1]
            if np.isnan(correlation):
                correlation = 0.0
        else:
            correlation = 0.0
        
        # Check for temporal precedence
        temporal_order_valid = self._check_temporal_precedence(cause_var, effect_var, lag)
        
        return {
            'cause': cause_var,
            'effect': effect_var,
            'lag': lag,
            'temporal_correlation': correlation,
            'temporal_order_valid': temporal_order_valid,
            'interpretation': self._interpret_temporal_correlation(correlation),
            'time_points_used': len(causes)
        }
    
    def _check_temporal_precedence(self, cause: str, effect: str, lag: int) -> bool:
        """Check if cause consistently precedes effect"""
        if len(self.timeline) < 2:
            return False
        
        valid_pairs = 0
        total_pairs = len(self.timeline) - lag
        
        for i in range(lag, len(self.timeline)):
            cause_val = self.timeline[i - lag].variables.get(cause)
            effect_val = self.timeline[i].variables.get(effect)
            
            if cause_val is not None and effect_val is not None:
                valid_pairs += 1
        
        return valid_pairs / max(1, total_pairs) > 0.7
    
    def _interpret_temporal_correlation(self, corr: float) -> str:
        """Interpret temporal correlation"""
        if corr > 0.6:
            return "STRONG_TEMPORAL_RELATIONSHIP"
        elif corr > 0.3:
            return "MODERATE_TEMPORAL_RELATIONSHIP"
        elif corr > 0.1:
            return "WEAK_TEMPORAL_RELATIONSHIP"
        elif corr < -0.1:
            return "NEGATIVE_TEMPORAL_RELATIONSHIP"
        else:
            return "NO_CLEAR_TEMPORAL_RELATIONSHIP"
    
    def forecast_with_intervention(self, variable: str,
                                 intervention: Dict[str, float],
                                 steps: int = 5) -> Dict:
        """Forecast future values given intervention"""
        
        if not self.timeline:
            raise ValueError("No historical data")
        
        # Simple linear extrapolation for demonstration
        # In production: causal state-space models, Bayesian structural time series
        
        historical = [state.variables.get(variable, 0) 
                     for state in self.timeline[-10:]]  # Last 10 points
        
        if len(historical) < 2:
            forecast = [historical[0] if historical else 0] * steps
        else:
            # Simple trend
            trend = (historical[-1] - historical[0]) / max(1, len(historical) - 1)
            last_value = historical[-1]
            
            forecast = []
            for i in range(1, steps + 1):
                # Apply intervention effect if variable is intervened on
                if variable in intervention:
                    forecasted = intervention[variable]
                else:
                    # Check if any cause of this variable is intervened on
                    intervened_effect = 0
                    for cause, value in intervention.items():
                        # Check if cause affects variable
                        effect_result = self.compute_temporal_effect(cause, variable, lag=1)
                        if effect_result['temporal_correlation'] > 0.3:
                            intervened_effect += value * effect_result['temporal_correlation'] * 0.5
                    
                    forecasted = last_value + (trend * i) + intervened_effect
                
                forecast.append(forecasted)
        
        return {
            'variable': variable,
            'intervention': intervention,
            'forecast_steps': steps,
            'forecast_values': forecast,
            'confidence_intervals': [
                (max(0, f - 0.1), f + 0.1) for f in forecast
            ],
            'method': 'temporal_causal_extrapolation'
        }
    
    def check_granger_causality(self, X: str, Y: str, max_lag: int = 3) -> Dict:
        """Test if X Granger-causes Y"""
        
        if len(self.timeline) < max_lag * 2:
            return {'result': 'INSUFFICIENT_DATA', 'p_value': 1.0}
        
        # Simplified implementation
        # In production: vector autoregression, F-test
        
        best_lag = 0
        best_correlation = 0
        
        for lag in range(1, max_lag + 1):
            result = self.compute_temporal_effect(X, Y, lag)
            corr = abs(result['temporal_correlation'])
            
            if corr > best_correlation:
                best_correlation = corr
                best_lag = lag
        
        # Simple "significance" based on correlation strength
        p_value = 1.0 - best_correlation  # Placeholder
        
        return {
            'null_hypothesis': f'{X} does not Granger-cause {Y}',
            'alternative': f'{X} Granger-causes {Y}',
            'best_lag': best_lag,
            'max_correlation': best_correlation,
            'p_value': p_value,
            'significant_at_0.05': p_value < 0.05,
            'interpretation': 'GRANGER_CAUSAL' if p_value < 0.05 else 'NO_GRANGER_CAUSALITY'
        }

# ============================================================================
# INTEGRATION WITH EXISTING AXIOMATIC SYSTEM
# ============================================================================

class ExtendedAxiomaticSystem:
    """Complete axiomatic system with intervention and temporal logic"""
    
    def __init__(self):
        from agi_axiom import AGI_Axiom  # Import existing system
        
        self.base_agi = AGI_Axiom()
        self.intervention_logic = InterventionLogic()
        self.temporal_logic = TemporalLogic()
        
        # Extend axiomatic base with causal-temporal axioms
        self._extend_axioms()
        
    def _extend_axioms(self):
        """Add intervention and temporal axioms to base system"""
        
        # These would be added to the AxiomaticBase
        intervention_axioms = [
            {
                'id': 'INTERVENTION_1',
                'type': 'CAUSAL',
                'statement': 'P(Y|do(X)) identifiable if backdoor criterion satisfied',
                'confidence': 0.95
            },
            {
                'id': 'INTERVENTION_2', 
                'type': 'CAUSAL',
                'statement': 'Counterfactuals require abduction of exogenous variables',
                'confidence': 0.92
            },
            {
                'id': 'TEMPORAL_1',
                'type': 'CAUSAL',
                'statement': 'Cause must precede effect in time',
                'confidence': 1.0
            },
            {
                'id': 'TEMPORAL_2',
                'type': 'CAUSAL',
                'statement': 'Granger causality tests temporal precedence with predictive power',
                'confidence': 0.88
            }
        ]
        
        # In production: actually add these to self.base_agi.axiomatic_base
    
    def causal_analysis(self, question: str, data: Dict = None) -> Dict:
        """Perform complete causal analysis with intervention and temporal reasoning"""
        
        # Step 1: Basic axiomatic reasoning
        base_result = self.base_agi.process(question)
        
        # Step 2: Extract causal structure from question
        causal_structure = self._extract_causal_structure(question)
        
        # Step 3: Apply intervention logic if relevant
        intervention_result = None
        if 'effect of' in question.lower() or 'if we' in question.lower():
            intervention_result = self._apply_intervention_logic(question, data)
        
        # Step 4: Apply temporal logic if time-related
        temporal_result = None
        if any(word in question.lower() for word in ['over time', 'future', 'forecast', 'trend']):
            temporal_result = self._apply_temporal_logic(question, data)
        
        return {
            'base_reasoning': base_result,
            'causal_structure': causal_structure,
            'intervention_analysis': intervention_result,
            'temporal_analysis': temporal_result,
            'integrated_certainty': self._integrate_certainty(
                base_result.get('certainty_score', 0),
                intervention_result,
                temporal_result
            ),
            'analysis_type': 'COMPLETE_CAUSAL_TEMPORAL'
        }
    
    def _extract_causal_structure(self, question: str) -> Dict:
        """Extract causal variables and relationships from question"""
        # Simple keyword extraction
        import re
        
        # Look for cause-effect patterns
        patterns = [
            r'effect of (\w+) on (\w+)',
            r'(\w+) causes (\w+)',
            r'impact of (\w+) on (\w+)',
            r'if (\w+) then (\w+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, question.lower())
            if match:
                return {
                    'cause': match.group(1),
                    'effect': match.group(2),
                    'pattern': pattern
                }
        
        return {'structure': 'UNKNOWN', 'certainty': 0.5}
    
    def _apply_intervention_logic(self, question: str, data: Dict) -> Dict:
        """Apply intervention/counterfactual logic"""
        # Create simple causal model based on question
        model_name = f"model_{hashlib.sha256(question.encode()).hexdigest()[:8]}"
        
        # Extract variables
        words = question.lower().split()
        variables = [w for w in words if len(w) > 3 and w not in 
                    ['what', 'would', 'effect', 'impact', 'cause', 'causal']]
        
        if len(variables) >= 2:
            model = self.intervention_logic.create_model(
                model_name,
                nodes=variables[:4],
                edges=[(variables[0], variables[1])]  # Simple chain
            )
            
            # Compute intervention effect
            result = self.intervention_logic.compute_intervention_effect(
                model_name,
                intervention={variables[0]: 1.0},
                outcome=variables[1]
            )
            
            return result
        
        return {'status': 'NO_CAUSAL_STRUCTURE_IDENTIFIED'}
    
    def _apply_temporal_logic(self, question: str, data: Dict) -> Dict:
        """Apply temporal causal logic"""
        if data and 'time_series' in data:
            # Add time points from data
            for ts_data in data['time_series']:
                self.temporal_logic.add_time_point(
                    variables=ts_data.get('variables', {}),
                    timestamp=datetime.fromisoformat(ts_data.get('timestamp', ''))
                )
            
            # Analyze temporal relationships
            if 'variables' in data:
                for var_pair in data.get('variable_pairs', []):
                    if len(var_pair) == 2:
                        result = self.temporal_logic.compute_temporal_effect(
                            var_pair[0], var_pair[1]
                        )
                        return result
        
        return {'status': 'NO_TEMPORAL_DATA_PROVIDED'}
    
    def _integrate_certainty(self, base_certainty: float, 
                           intervention_result: Dict, 
                           temporal_result: Dict) -> float:
        """Integrate certainty from multiple analysis methods"""
        certainties = [base_certainty]
        
        if intervention_result and 'certainty' in intervention_result:
            certainties.append(intervention_result['certainty'])
        
        if temporal_result and 'temporal_correlation' in temporal_result:
            # Convert correlation to certainty-like measure
            corr = abs(temporal_result['temporal_correlation'])
            certainties.append(corr)
        
        return np.mean(certainties) if certainties else base_certainty

# ============================================================================
# DEMONSTRATION
# ============================================================================

if __name__ == "__main__":
    print("ðŸ§  EXTENDED AXIOMATIC SYSTEM - INTERVENTION & TEMPORAL LOGIC")
    print("=" * 60)
    
    # Create extended system
    extended_system = ExtendedAxiomaticSystem()
    
    # Example 1: Intervention logic
    print("\n1. INTERVENTION LOGIC DEMONSTRATION")
    print("-" * 40)
    
    intervention_logic = InterventionLogic()
    
    # Create a causal model
    model = intervention_logic.create_model(
        "medical_trial",
        nodes=["treatment", "recovery", "age", "severity"],
        edges=[
            ("treatment", "recovery"),
            ("age", "recovery"),
            ("severity", "recovery"),
            ("age", "severity")
        ]
    )
    
    # Compute intervention effect
    effect = intervention_logic.compute_intervention_effect(
        "medical_trial",
        intervention={"treatment": 1.0},
        outcome="recovery",
        conditions={"age": 45, "severity": 0.7}
    )
    
    print(f"Causal effect of treatment on recovery: {effect['causal_effect']:.3f}")
    print(f"Interpretation: {effect['interpretation']}")
    print(f"Hash: {effect['intervention_record']['hash']}")
    
    # Example 2: Temporal logic
    print("\n2. TEMPORAL LOGIC DEMONSTRATION")
    print("-" * 40)
    
    temporal_logic = TemporalLogic()
    
    # Add time series data
    for i in range(10):
        temporal_logic.add_time_point({
            "price": 100 + i * 2 + np.random.normal(0, 1),
            "demand": 50 - i * 0.5 + np.random.normal(0, 2),
            "supply": 30 + i * 1.5
        })
    
    # Check Granger causality
    granger_result = temporal_logic.check_granger_causality("price", "demand")
    print(f"Granger causality test:")
    print(f"  H0: {granger_result['null_hypothesis']}")
    print(f"  p-value: {granger_result['p_value']:.4f}")
    print(f"  Significant: {granger_result['significant_at_0.05']}")
    print(f"  Interpretation: {granger_result['interpretation']}")
    
    # Example 3: Integrated analysis
    print("\n3. INTEGRATED CAUSAL-TEMPORAL ANALYSIS")
    print("-" * 40)
    
    question = "What is the effect of interest rate changes on inflation over time?"
    
    analysis = extended_system.causal_analysis(
        question,
        data={
            'time_series': [
                {'variables': {'interest_rate': 2.5, 'inflation': 3.1}},
                {'variables': {'interest_rate': 3.0, 'inflation': 2.8}}
            ]
        }
    )
    
    print(f"Question: {question}")
    print(f"Base reasoning certainty: {analysis['base_reasoning']['certainty_score']:.1%}")
    print(f"Causal structure: {analysis.get('causal_structure', {}).get('cause', 'unknown')} â†’ {analysis.get('causal_structure', {}).get('effect', 'unknown')}")
    print(f"Integrated certainty: {analysis['integrated_certainty']:.1%}")
    print(f"Analysis type: {analysis['analysis_type']}")
    
    print("\n" + "=" * 60)
    print("âœ… INTERVENTION_LOGIC and TEMPORAL_LOGIC implementation complete")
    print("   These modules extend the axiomatic base with:")
    print("   â€¢ Pearl's do-calculus for interventions")
    print("   â€¢ Counterfactual reasoning (abduction-action-prediction)")
    print("   â€¢ Temporal causal dynamics")
    print("   â€¢ Granger causality testing")
    print("   â€¢ Integrated causal-temporal analysis")
    print("=" * 60)
